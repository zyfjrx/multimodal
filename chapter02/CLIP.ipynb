{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:45.133217Z",
     "start_time": "2025-10-16T05:39:45.128206Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:47.204734Z",
     "start_time": "2025-10-16T05:39:47.196006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 位置嵌入\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, width, max_seq_length):\n",
    "        super().__init__()\n",
    "        # 创建一个 (token的数量, 嵌入的维度) 形状的全0张量\n",
    "        # width 就是 d_model\n",
    "        pe = torch.zeros(max_seq_length, width)\n",
    "        # 将位置编码信息填充到pe中\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(width):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
    "        # 位置编码信息进行冻结，不参与反向传播\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 直接将位置编码和token的嵌入进行相加\n",
    "        x = x + self.pe\n",
    "        return x"
   ],
   "id": "6ecda0d668023f92",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:49.427102Z",
     "start_time": "2025-10-16T05:39:49.417912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 注意力头\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, width, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(width, head_size)\n",
    "        self.key = nn.Linear(width, head_size)\n",
    "        self.value = nn.Linear(width, head_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 计算K，Q，V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Q和K的点积\n",
    "        attention = Q @ K.transpose(-2,-1)\n",
    "        # 缩放\n",
    "        attention = attention / (self.head_size ** 0.5)\n",
    "        # 掩码\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        attention = attention @ V\n",
    "        return attention"
   ],
   "id": "46dc745df0affe12",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:51.493708Z",
     "start_time": "2025-10-16T05:39:51.486221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, width, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = width // n_heads\n",
    "        self.W_o = nn.Linear(width, width)\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(width, self.head_size) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 拼接多个注意力头\n",
    "        out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "        return out"
   ],
   "id": "5aa1df6fb84f4b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:53.419712Z",
     "start_time": "2025-10-16T05:39:53.410796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transformer编码器\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width # 嵌入的维度d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        # 多头注意力\n",
    "        self.mha = MultiHeadAttention(width, n_heads)\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width*r_mlp, self.width)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ],
   "id": "4775651674dffe7c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:39:57.932969Z",
     "start_time": "2025-10-16T05:39:57.923238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 文本分词器\n",
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # 添加 SOT token 和 EOT token\n",
    "        out = out + \"\".join([\n",
    "            chr(0) for _ in range(max_seq_length-len(out))\n",
    "        ]) # 添加Padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # 对文本进行编码\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((\n",
    "            mask,\n",
    "            torch.zeros(max_seq_length - len(mask))\n",
    "        )).type(torch.IntTensor)\n",
    "    else:\n",
    "        # 将input_ids解码为text文本\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask"
   ],
   "id": "64892b34ee52b0ab",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:40:00.166583Z",
     "start_time": "2025-10-16T05:40:00.154598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, # 词汇表大小=256\n",
    "        width, # 宽度d_model\n",
    "        max_seq_length, # 文本最大长度\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        emb_dim # 嵌入维度\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width)\n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            width,\n",
    "            max_seq_length\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(width, n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        # 可学习投影（projection）\n",
    "        # d_model(width) --- emb_dim\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        # 文本嵌入\n",
    "        x = self.encoder_embedding(text)\n",
    "        # 位置嵌入\n",
    "        x = self.positional_embedding(x)\n",
    "        # Transformer编码器\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "        # 从EOT的嵌入抽取特征\n",
    "        x = x[\n",
    "            torch.arange(text.shape[0]), # 批次中数据的索引\n",
    "            # 取出掩码mask矩阵的第0行，加和再减1，就得到了EOT的索引\n",
    "            torch.sub(torch.sum(mask[:,0],dim=1),1)\n",
    "        ]\n",
    "        # 将文本特征嵌入到联合嵌入空间中（多模态嵌入空间）\n",
    "        # 文本编码器输出的张量的维度和图像编码器输出的张量的维度必须一致\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x"
   ],
   "id": "526be29e9db1a69e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:40:34.854759Z",
     "start_time": "2025-10-16T05:40:34.839748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width, # 补丁嵌入的维度d_model\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        emb_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0  \\\n",
    "           and img_size[1] % patch_size[1] == 0, \\\n",
    "           \"img_size必须能被patch_size整除\"\n",
    "        assert width % n_heads == 0, \\\n",
    "           \"width必须能被n_heads整除\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) \\\n",
    "                      // (patch_size[0] * patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            n_channels,\n",
    "            width,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "        self.positional_embedding = PositionalEmbedding(\n",
    "            width,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(width,n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # 可学习的投影\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 补丁嵌入\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # 位置嵌入\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer编码器\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # 获取类别token\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # 多模态嵌入\n",
    "        # 保证文本编码器的输出的维度和图像编码器的输出的维度相等\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x"
   ],
   "id": "21563dc23ab81d3d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:40:56.480464Z",
     "start_time": "2025-10-16T05:40:56.467440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim, # 经过可学习投影后的嵌入维度\n",
    "        vit_width, # 图像编码器的宽度(d_model)\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        vit_layers,\n",
    "        vit_heads,\n",
    "        vocab_size,\n",
    "        text_width, # 文本编码器的宽度（d_model）\n",
    "        max_seq_length,\n",
    "        text_heads,\n",
    "        text_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(\n",
    "            vit_width,\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            n_channels,\n",
    "            vit_layers,\n",
    "            vit_heads,\n",
    "            emb_dim\n",
    "        )\n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size,\n",
    "            text_width,\n",
    "            max_seq_length,\n",
    "            text_heads,\n",
    "            text_layers,\n",
    "            emb_dim\n",
    "        )\n",
    "        # 可学习温度\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self, image, text, mask=None):\n",
    "        # Iₑ是图像嵌入，形状 [B, D=emb_dim]\n",
    "        I_e = self.image_encoder(image)\n",
    "        # Tₑ是文本嵌入，形状 [B, D=emb_dim]\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        # 缩放逐点余弦相似度[n, n]\n",
    "        # 形状 I_e @ T_e^T : [B, D] @ [D, B] --> [B, B]\n",
    "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
    "\n",
    "        # 对称损失函数 labels形状为[B]，值为 [0, 1, 2, ..., B-1]\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "        # 从文本 --> 图像方向，以文本嵌入 T₃ 为例子，\n",
    "        # 交叉熵损失的目标是让 T₃⋅I₃ 越大越好\n",
    "        loss_i = nn.functional.cross_entropy(\n",
    "            logits.transpose(-2,-1),\n",
    "            labels\n",
    "        )\n",
    "        # 从图像 --> 文本方向，以图像嵌入 I₃ 为例子，\n",
    "        # 交叉熵损失的目标是让 I₃⋅T₃ 越大越好\n",
    "        loss_t = nn.functional.cross_entropy(\n",
    "            logits,\n",
    "            labels\n",
    "        )\n",
    "        # 两个方向的损失求平均值\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ],
   "id": "b3e584ce4d5c1e3b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:03:02.252698Z",
     "start_time": "2025-10-16T06:03:02.242337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = load_dataset(\"/root/zhangyf/multimodal/datasets/clip-mnist/\")\n",
    "        self.transform = T.ToTensor()\n",
    "        if train:\n",
    "            self.split = \"train\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "\n",
    "        self.captions = {\n",
    "            0: \"An image of Zero\",\n",
    "            1: \"An image of One\",\n",
    "            2: \"An image of Two\",\n",
    "            3: \"An image of Three\",\n",
    "            4: \"An image of Four\",\n",
    "            5: \"An image of Five\",\n",
    "            6: \"An image of Six\",\n",
    "            7: \"An image of Seven\",\n",
    "            8: \"An image of Eight\",\n",
    "            9: \"An image of Nine\"\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[self.split]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 取出第i张图片\n",
    "        img = self.dataset[self.split][i][\"image\"]\n",
    "        # 转换成张量\n",
    "        img = self.transform(img)\n",
    "        # 图片对应的文本，以及掩码\n",
    "        cap, mask = tokenizer(\n",
    "           self.captions[self.dataset[self.split][i][\"label\"]]\n",
    "        )\n",
    "        # 为什么要repeat？\n",
    "        mask = mask.repeat(len(mask), 1)\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
   ],
   "id": "a302269b97c4b983",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:03:04.745003Z",
     "start_time": "2025-10-16T06:03:04.739111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emb_dim = 32 # 文本编码器和图像编码器输出的张量的维度\n",
    "vit_width = 9 # 图像编码器的嵌入的宽度\n",
    "img_size = (28,28)\n",
    "patch_size = (14,14)\n",
    "n_channels = 1\n",
    "vit_layers = 3 # 图像编码器中编码器的层的数量\n",
    "vit_heads = 3 # 图像编码器中注意力头的数量\n",
    "vocab_size = 256 # 词汇表大小\n",
    "text_width = 32 # 文本编码器的嵌入的宽度\n",
    "max_seq_length = 32 # 最大序列长度\n",
    "text_heads = 8 # 文本编码器中注意力头的数量\n",
    "text_layers = 4 # 文本编码器中编码器的层数\n",
    "lr = 1e-3 # 学习率\n",
    "epochs = 10\n",
    "batch_size = 128"
   ],
   "id": "421c7f16dfbad539",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:03:06.661245Z",
     "start_time": "2025-10-16T06:03:06.561637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_set = MNIST(train = True)\n",
    "test_set = MNIST(train = False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ],
   "id": "28ae2143b2e8b0f9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T06:16:38.263242Z",
     "start_time": "2025-10-16T06:04:15.284686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img = data[\"image\"].to(device)\n",
    "        cap = data[\"caption\"].to(device)\n",
    "        mask = data[\"mask\"].to(device)\n",
    "        loss = model(img, cap, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"./clip.pt\")\n",
    "        print(\"模型已经保存.\")"
   ],
   "id": "56f507915bec0c84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch Loss: 2.809\n",
      "模型已经保存.\n",
      "Epoch [2/10], Batch Loss: 2.617\n",
      "模型已经保存.\n",
      "Epoch [3/10], Batch Loss: 2.526\n",
      "模型已经保存.\n",
      "Epoch [4/10], Batch Loss: 2.686\n",
      "Epoch [5/10], Batch Loss: 2.418\n",
      "模型已经保存.\n",
      "Epoch [6/10], Batch Loss: 2.417\n",
      "模型已经保存.\n",
      "Epoch [7/10], Batch Loss: 2.449\n",
      "Epoch [8/10], Batch Loss: 2.443\n",
      "Epoch [9/10], Batch Loss: 2.390\n",
      "模型已经保存.\n",
      "Epoch [10/10], Batch Loss: 2.391\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T07:01:53.803505Z",
     "start_time": "2025-10-16T07:01:43.753486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载最好的模型\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"./clip.pt\", map_location=device))\n",
    "\n",
    "# 获取数据集的标签和图片进行对比\n",
    "text = torch.stack(\n",
    "    [tokenizer(x)[0] for x in test_set.captions.values()]\n",
    ").to(device)\n",
    "mask = torch.stack(\n",
    "    [tokenizer(x)[1] for x in test_set.captions.values()]\n",
    ")\n",
    "mask = mask.repeat(\n",
    "    1,\n",
    "    len(mask[0])\n",
    ").reshape(\n",
    "    len(mask),\n",
    "    len(mask[0]),\n",
    "    len(mask[0])\n",
    ").to(device)\n",
    "\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # 图像\n",
    "        images = data[\"image\"].to(device)\n",
    "        # 文本\n",
    "        labels = data[\"caption\"].to(device)\n",
    "        # 使用clip模型中的图像编码器对图像抽取特征\n",
    "        image_features = model.image_encoder(images)\n",
    "        # 使用clip模型中的文本编码器对文本抽取特征\n",
    "        text_features = model.text_encoder(text, mask=mask)\n",
    "        # 归一化\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        # I_e @ T_e^T\n",
    "        similarity = (\n",
    "            100.0 * image_features @ text_features.T\n",
    "        ).softmax(dim=-1)\n",
    "        _, indices = torch.max(similarity, 1)\n",
    "        # 预测结果\n",
    "        pred = torch.stack([\n",
    "            tokenizer(test_set.captions[int(i)])[0]\n",
    "            for i in indices\n",
    "        ]).to(device)\n",
    "        # 预测正确的样本数量\n",
    "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
    "        total += len(labels)\n",
    "\n",
    "print(f'\\n预测准确率: {100 * correct // total} %')"
   ],
   "id": "994fbe96295a5d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测准确率: 95 %\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T07:07:52.013142Z",
     "start_time": "2025-10-16T07:07:51.622338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载模型\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"./clip.pt\", map_location=device))\n",
    "\n",
    "# 标题\n",
    "class_names = [\n",
    "    \"a photo of 0\",\n",
    "    \"an image of one\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"nine\",\n",
    "    \"Trump\",\n",
    "    \"Musk\"\n",
    "]\n",
    "\n",
    "text = torch.stack(\n",
    "    [tokenizer(x)[0] for x in class_names]\n",
    ").to(device)\n",
    "mask = torch.stack(\n",
    "    [tokenizer(x)[1] for x in class_names]\n",
    ")\n",
    "mask = mask.repeat(\n",
    "    1,\n",
    "    len(mask[0])\n",
    ").reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "idx = 1000\n",
    "# 去测试数据集中的第1000张图片\n",
    "img = test_set[idx][\"image\"][None,:]\n",
    "plt.imshow(img[0].permute(1, 2, 0), cmap=\"gray\")\n",
    "# 将图片的标题文本展示，例如\"An Image Of Nine\"\n",
    "plt.title(tokenizer(\n",
    "    test_set[idx][\"caption\"],\n",
    "    encode=False,\n",
    "    mask=test_set[idx][\"mask\"][0]\n",
    ")[0])\n",
    "plt.show()\n",
    "img = img.to(device)\n",
    "with torch.no_grad():\n",
    "    # 抽取图片的特征\n",
    "    image_features = model.image_encoder(img)\n",
    "    # 抽取文本的特征\n",
    "    text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# 计算第1000张图片和所有文本的相似度\n",
    "similarity = (\n",
    "    100.0 * image_features @ text_features.T\n",
    ").softmax(dim=-1)\n",
    "# 返回所有文本中和图片特征最相似的5个文本\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n预测结果:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
   ],
   "id": "5093c3f5ddc82765",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测结果:\n",
      "\n",
      "            nine: 85.36%\n",
      " an image of one: 14.53%\n",
      "            Musk: 0.12%\n",
      "    a photo of 0: 0.00%\n",
      "           Trump: 0.00%\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
